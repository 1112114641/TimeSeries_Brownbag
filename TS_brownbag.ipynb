{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from fbprophet import Prophet\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import scipy\n",
    "\n",
    "df = pd.read_csv(r'temperature.csv', parse_dates=[0], header=[0], index_col=[0])\n",
    "df.columns = ['Temperature (°C)']\n",
    "df['abs'] = range(len(df))\n",
    "df.index = df.index + 3*(pd.to_timedelta(99999999999990000)-pd.to_timedelta('09:46:39.999990'))*3\n",
    "df['linear'] = 0.0013246575342465753*df['abs']-0.0100212592764251\n",
    "df['sine'] = 6*np.sin(df['abs']*np.pi/(182.5)+14)+13\n",
    "df['added'] = df['linear'] + df['sine']\n",
    "df['noise'] = 0.6*(df[\"Temperature (°C)\"]-df['linear']-df['sine'])\n",
    "df['ds'] = df.index\n",
    "df['y'] = df['Temperature (°C)']\n",
    "df['homoscedasticity'] = df.iloc[:,0].rolling(window=30).std()\n",
    "\n",
    "# unfortunately the averaging is some work -.-\n",
    "def averaging(row):\n",
    "  try: \n",
    "    # average over the last four years:\n",
    "    # Also, why on earth would you exclude the month/year timedelta???\n",
    "    # If you are so bothered by it, do pay attention to the 30.5 days the average month has\n",
    "    # and the 365.25days the average year has oO!\n",
    "    avg = np.mean([df.loc[row.name-pd.Timedelta(x*365, unit=\"d\")][0] for x in range(5)])\n",
    "  except KeyError as e:\n",
    "    # NAN if 4 years haven't passed yet\n",
    "    return np.NaN\n",
    "  return avg\n",
    "df['averaging'] =  df.apply(averaging, axis=1)\n",
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To obtain the noise description - a normal distribution fit:\n",
    "_=plt.hist(df['noise'], bins=25, density=True, alpha=0.6, color='g')\n",
    "from scipy.stats import norm\n",
    "mu, std = norm.fit(df['noise'])\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = norm.pdf(x, mu, std)\n",
    "_=plt.plot(x, p, 'k', linewidth=2)\n",
    "_=plt.title(f\"Fit results: mu = {mu:.2f},  std = {std:.2f}\")\n",
    "# 2 * std = [5,95] confidence interval:\n",
    "lower_bound = df['added'] - 2*std\n",
    "upper_bound = df['added'] + 2*std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\")\n",
    "plt.plot(figsize=(6,6))\n",
    "sns.lineplot(x=df.index, y=df['Temperature (°C)'], data=df, label='Real Data')\n",
    "# sns.lineplot(x=df.index, y=df['averaging'], data=df, label='Averaging')\n",
    "# sns.lineplot(x=df.index, y=df['homoscedasticity'], data=df, label='30d Variance')\n",
    "# sns.lineplot(x=df.index, y=df['linear'], data=df, label='Linear Trend')\n",
    "# sns.lineplot(x=df.index, y=df['sine'], data=df, label='Periodic Part')\n",
    "sns.lineplot(x=df.index, y=df['added'], data=df, label='Model')\n",
    "plt.fill_between(df.index, lower_bound, upper_bound, alpha=.3, label='std', color=\"orange\")\n",
    "plt.ylim(-1,30.5)\n",
    "plt.ylabel('Temperature (°C)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing that the first discrete difference can get rid of all seasonality&trend:\n",
    "sns.lineplot(x=df.index, y=df['Temperature (°C)'], data=df, label='Real Data')\n",
    "sns.lineplot(x=df.index, y=df['Temperature (°C)'].diff(1), data=df, label='Discrete Difference')\n",
    "plt.ylim(-10,30.5)\n",
    "plt.ylabel('Temperature (°C)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance (30d Variance) and mean (noise) plot to show homoscedasticity:\n",
    "sns.set(style=\"darkgrid\")\n",
    "plt.plot(figsize=(6,6))\n",
    "sns.lineplot(x=df.index, y=df['noise'], data=df, label='Residual Noise')\n",
    "sns.lineplot(x=df.index, y=df['sine'], data=df, label='Seasonality Fit')\n",
    "sns.lineplot(x=df.index, y=df['homoscedasticity'], data=df, label='30d Variance')\n",
    "plt.ylabel('Temperature (°C)')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylim(-7,24.5)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Getting started on the facebook Prophet modelling:\n",
    "m = Prophet()\n",
    "m.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating the necessary in;ut cols for prophet's modelling:\n",
    "future = m.make_future_dataframe(periods=365)\n",
    "future.tail()\n",
    "forecast = m.predict(future)\n",
    "forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\n",
    "fig1 = m.plot(forecast)\n",
    "sns.set(style=\"darkgrid\")\n",
    "plt.plot(figsize=(6,6))\n",
    "plt.ylabel('Temperature (°C)')\n",
    "plt.legend(['Data','Fit & CI'],loc='upper right')\n",
    "plt.ylim(-1,30.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# component plot:\n",
    "sns.set(style=\"darkgrid\")\n",
    "fig2 = m.plot_components(forecast)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multi var modelling hereafter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the australian stonks market:\n",
    "stonks = pd.read_csv(r'AXJO.csv')\n",
    "stonks['Date'] = pd.to_datetime(stonks['Date'])\n",
    "stonks.index = stonks['Date']\n",
    "stonks['ds'] = stonks['Date']\n",
    "stonks['y'] = stonks['Open']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(figsize=(6,6))\n",
    "sns.set(style=\"darkgrid\")\n",
    "ax1 = stonks['Open'].plot(label='ASX200 Opening Price')\n",
    "ax2 = (stonks['Volume']/1000).plot(label='Volume', secondary_y=True)\n",
    "ax1.set_ylabel('ASX200 Open Price in A$')\n",
    "ax1.yaxis.label.set_color('blue')\n",
    "ax2.set_ylabel('Trade Volume (1k)')\n",
    "ax2.yaxis.label.set_color('orange')\n",
    "# ax2.yaxis.label.set_scolor('orange')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m = Prophet()\n",
    "m.fit(stonks[stonks['Date']<'2020-02-10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future = m.make_future_dataframe(periods=365)\n",
    "future.tail()\n",
    "forecast = m.predict(future)\n",
    "forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "m.plot(forecast)\n",
    "plt.ylabel('ASX200 Opening Prices(A$)')\n",
    "plt.legend(['Data','Fit & CI'],loc='top left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\")\n",
    "fig2 = m.plot_components(forecast)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The little brother of Cross-correlation: autocorrelation_plot!\n",
    "# autocorrelation_plot is really helpful to determin the best values to build discrete differences\n",
    "# if, e.g., autocorrelation_plot(df['noise']) shows a massive dependence on lag=15, then one should use it\n",
    "# as a discrete difference step along the lines of df['noise'].diff(15)\n",
    "autocorrelation_plot(df['noise'], label='acf of noise')\n",
    "_ = plt.xlabel('Lag (days)')\n",
    "_ = plt.ylim((-.3,.3))\n",
    "# plt.specgram(df['y'].dropna(),fs=185)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same plot for 1st 10 days\n",
    "plt.acorr(df['noise'], label='Autocorrelations <10d')#\n",
    "plt.legend()\n",
    "plt.xlim(-0.05,10.05)\n",
    "# df['noise'].autocorr(80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same for the \"stonks\" ;)\n",
    "autocorrelation_plot(stonks['Open'], label='Open')\n",
    "autocorrelation_plot(stonks['Volume'], label='Volume')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some of the analysis tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "# loading data:\n",
    "\n",
    "df_all = pd.read_csv('./data/df_all.csv', header=[0], index_col=[0],parse_dates=[0])            \n",
    "df_all = df_all.sort_index()\n",
    "df_all.index = pd.to_datetime(df_all.index)\n",
    "# silly gluonTS needs an index with freq set:\n",
    "df_all.index = pd.date_range(start=df_all.index[0], freq='120min', periods=df_all.shape[0])\n",
    "\n",
    "\n",
    "y = pd.read_csv('./data/y.csv', header=[0], index_col=[0],parse_dates=[0])          \n",
    "y = y.sort_index()\n",
    "y.index = pd.to_datetime(y.index)\n",
    "y.index = pd.date_range(start=y.index[0], freq='120min', periods=y.shape[0])\n",
    "\n",
    "# reality check:\n",
    "assert all(dat in df_all.index for dat in y.index), 'Index differs for df_all and y!'\n",
    "\n",
    "print('df_all.shape: ',df_all.shape)\n",
    "print('y.shape: ',y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "temp = df_all[['temperature_SA','windSpeed_SA','cloudCover_SA','scheduledGeneration_SA']\n",
    "              ].merge(y['price_SA'], \n",
    "                      left_index=True, \n",
    "                      right_index=True).copy()\n",
    "\n",
    "cor = temp.corr()\n",
    "\n",
    "# plot the heatmap:\n",
    "plt.figure(figsize=(6,4))\n",
    "_ = sns.heatmap(cor,\n",
    "                xticklabels=cor.columns,\n",
    "                yticklabels=cor.columns,\n",
    "                annot=True,\n",
    "                fmt='.1f')\n",
    "_ = plt.xticks(rotation=90)\n",
    "_ = plt.yticks(rotation=45)\n",
    "del temp\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right, let's get started with the (cross-) correlations:\n",
    "def crosscorr(df_x, df_y, lags=[0]):\n",
    "    # calculates cross correlation\n",
    "    cross_corr = [df_x.corr(df_y.shift(lag)) for lag in lags]\n",
    "    return cross_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df_all[['pressure_SA','temperature_SA',               \n",
    "               'rainProb_SA','windSpeed_SA',\n",
    "               'windDirection_SA','cloudCover_SA',\n",
    "               'scheduledGeneration_SA']].merge(y['price_SA'],\n",
    "                                                left_index=True,\n",
    "                                                right_index=True).copy()\n",
    "\n",
    "abc = {col: crosscorr(temp[col],temp['price_SA'],range(50)) for col in temp.columns if col!='price_SA'}\n",
    "abc = pd.DataFrame(abc)\n",
    "sns.set(style=\"darkgrid\")\n",
    "abc.index.name = 'Lag (h)'\n",
    "\n",
    "sns.heatmap(abc.T, robust=True)#, cmap=cmap)\n",
    "plt.plot(figsize=(6,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do be careful with correlations please, they are not the catch-all that will bring you world peace!\n",
    "a = np.arange(-15,15)\n",
    "b = np.arange(-15,15)**2\n",
    "c = np.arange(-25,25)\n",
    "d = np.arange(-25,25)**2\n",
    "\n",
    "ab = pd.DataFrame({'x':a,'y':b})\n",
    "cd = pd.DataFrame({'x':c,'y':d})\n",
    "corr1 = ab.corr()\n",
    "corr2 = cd.corr()\n",
    "\n",
    "_=sns.set(style=\"darkgrid\")\n",
    "_=plt.plot(c,d,label='[-25,25]')\n",
    "_=plt.plot(a,b,label='[-15,15]')\n",
    "_=plt.legend(loc='best')\n",
    "_=plt.title(f'Correlation [-15,15]: {corr1.iloc[0,1]:.2f} - Correlation [-25,25]: {corr2.iloc[0,1]:.2f}')\n",
    "_=plt.plot(figsize=(6,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Right, now for the modelling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's kick off with gluonTS: (caveat: may be a super nice package, defo not as performant yet in pure regression in comp to e.g. tf/torch)\n",
    "#\n",
    "# Do change the epoch number to see slightly better results\n",
    "#\n",
    "from gluonts.dataset import common\n",
    "from gluonts.dataset.field_names import FieldName\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.model import deepar, wavenet\n",
    "from gluonts.trainer import Trainer\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "import os\n",
    "from tqdm.autonotebook import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "prediction_length = 24\n",
    "start = pd.Timestamp('2019-11-11 03:00:00') # df_all.index[0]\n",
    "freq = '120min'\n",
    "\n",
    "df_train = df_all[['scheduledGeneration_SA','temperature_SA', 'rainProb_SA',\n",
    "       'windSpeed_SA', 'cloudCover_SA']]\n",
    "\n",
    "train_ds = ListDataset([{FieldName.TARGET: \n",
    "                                    y.iloc[:-prediction_length, 2].values, \n",
    "                         FieldName.FEAT_DYNAMIC_REAL: \n",
    "                                    df_train.iloc[:-prediction_length,:].values,\n",
    "                         FieldName.START: \n",
    "                                    start}],\n",
    "                       freq=freq)\n",
    "# test dataset: use the whole dataset, add \"target\" and \"start\" fields\n",
    "test_ds = ListDataset([{FieldName.TARGET: \n",
    "                                    y.iloc[:, 2].values, \n",
    "                        FieldName.FEAT_DYNAMIC_REAL: \n",
    "                                    df_train.values,\n",
    "                        FieldName.START: \n",
    "                                    start}],\n",
    "                       freq=freq)\n",
    "\n",
    "\n",
    "trainer = Trainer(epochs=10)\n",
    "estimator = deepar.DeepAREstimator(\n",
    "    freq=\"120min\", prediction_length=prediction_length, trainer=trainer)\n",
    "predictor = estimator.train(training_data=train_ds)\n",
    "\n",
    "prediction = next(predictor.predict(test_ds))\n",
    "# print(prediction.mean)\n",
    "# prediction.plot(output_file='graph.png')\n",
    "\n",
    "forecast_it, ts_it = make_evaluation_predictions(\n",
    "    dataset=test_ds,  # test dataset\n",
    "    predictor=predictor,  # predictor\n",
    "    num_samples=100,  # number of sample paths we want for evaluation\n",
    ")\n",
    "forecasts = list(forecast_it)\n",
    "tss = list(ts_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As one can tell, the model still leaves room for improvement:\n",
    "def plot_prob_forecasts(ts_entry, forecast_entry):\n",
    "    # plot everything\n",
    "    sns.set(style=\"darkgrid\")\n",
    "\n",
    "    plot_length = 50\n",
    "    prediction_intervals = (50.0, 90.0)\n",
    "    legend = [\"observations\", \"median prediction\"] + [f\"{k}% prediction interval\" for k in prediction_intervals][::-1]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 7))\n",
    "    ts_entry[-plot_length:].plot(ax=ax)  # plot the time series\n",
    "    forecast_entry.plot(prediction_intervals=prediction_intervals, color='g')\n",
    "    plt.grid(which=\"both\")\n",
    "    plt.legend(legend, loc=\"upper left\")\n",
    "    \n",
    "plot_prob_forecasts(tss[0], forecasts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WaveNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Pick and mix your input variables (`df_train`) to tune model performance!\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from tcn import TCN, tcn_full_summary\n",
    "\n",
    "df_train = df_all[['temperature_SA','cloudCover_SA','windSpeed_SA','scheduledGeneration_SA']]\n",
    "batch_size, timesteps, input_dim = None, df_train.shape[1], 1\n",
    "\n",
    "i = Input(batch_shape=(batch_size, timesteps, input_dim))\n",
    "o = TCN(kernel_size=2,\n",
    "        dilations=[1, 2, 4, 8, 16, 32],\n",
    "        return_sequences=True)(i)\n",
    "o = TCN(kernel_size=2,\n",
    "        dilations=[1, 2, 4, 8, 16, 32],\n",
    "        return_sequences=False)(o)\n",
    "o = Dense(1)(o)\n",
    "\n",
    "m3 = Model(inputs=[i], outputs=[o])\n",
    "\n",
    "m3.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "\n",
    "x_train = np.array(df_train.iloc[:-25,:].values.reshape(-1,df_train.shape[1],1))\n",
    "y_train = np.array(y.iloc[:-25,2].values.reshape(-1,1))\n",
    "\n",
    "\n",
    "name_of_run = '1'\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=150)\n",
    "mc = ModelCheckpoint(f'./models/run_{name_of_run}/', monitor='val_loss',\n",
    "                     verbose=0, save_best_only=True,\n",
    "                     save_weights_only=False, mode='auto')\n",
    "\n",
    "m3.fit(x_train, y_train, epochs=2000, validation_split=0.15, callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# m3.save('./models/model_WaveNet_500epochs-2layer-extend-ES/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot val-/ train- loss\n",
    "plt.plot(m3.history.history['loss'], label='train')\n",
    "plt.plot(m3.history.history['val_loss'], label='test')\n",
    "plt.legend(loc='best')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the fit results together with real data:\n",
    "\n",
    "import tensorflow \n",
    "\n",
    "# m3 = tensorflow.keras.models.load_model('./models/model_WaveNet_500epochs-2layer-extend-ES/')\n",
    "\n",
    "m4 = tensorflow.keras.models.load_model(f'./models/run_{name_of_run}')\n",
    "\n",
    "test_series_x = np.array(df_train.iloc[-126:-100,:].values.reshape(-1,df_train.shape[1],1))\n",
    "\n",
    "preds_m3 = m3.predict(test_series_x)\n",
    "preds_m4 = m4.predict(test_series_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_series_y = y.iloc[-175:-125,2]\n",
    "train_series_y = y.iloc[-126:-100,2]\n",
    "\n",
    "pred_y = pd.DataFrame({\"Model_0\": preds_m3.ravel(),\n",
    "                  \"Model_1 - best of 0\": preds_m4.ravel()},\n",
    "                  index=train_series_y.index\n",
    "                  )\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(10, 7))\n",
    "train_series_y.plot(ax=ax)\n",
    "plt.axvline(test_series_y.index[-1], color='black')\n",
    "\n",
    "test_series_y.plot(ax=ax)\n",
    "\n",
    "test_series_x = np.array(df_all.iloc[-126:-100,:].values.reshape(26,21,1))\n",
    "idx = df_all.iloc[-126:-100,:].index\n",
    "idx.freq=None\n",
    "\n",
    "# pred_y['Model_0'].plot(ax=ax)\n",
    "pred_y['Model_1 - best of 0'].plot(ax=ax)\n",
    "plt.ylabel('AUD/MWh')\n",
    "plt.legend([\"Test series\", \"End of train series\", 'Train series','WaveNet: Best of 500 Epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37464bitbaseconda42f0b70a5ff844178a680616f3fb051d",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}